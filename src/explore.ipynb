{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                                                 url  is_spam\n",
                        "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
                        "1                             https://www.hvper.com/     True\n",
                        "2                 https://briefingday.com/m/v4n3i4f3     True\n",
                        "3   https://briefingday.com/n/20200618/m#commentform    False\n",
                        "4                        https://briefingday.com/fan     True\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# URL of the dataset\n",
                "url = \"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\"\n",
                "\n",
                "# Load the dataset into a pandas DataFrame\n",
                "df = pd.read_csv(url)\n",
                "\n",
                "# Display the first few rows of the DataFrame to ensure it's loaded correctly\n",
                "print(df.head())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n",
                        "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import nltk\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1569    [http, www, morningbrew, com, daily, story, 20...\n",
                        "2229    [http, www, morningbrew, com, daily, story, 20...\n",
                        "2296    [http, www, nytimes, com, article, maskne, acn...\n",
                        "1800    [http, podcasts, apple, com, u, podcast, foxy,...\n",
                        "1273    [http, www, nycpride, org, event, nyc, pride, ...\n",
                        "Name: processed_url, dtype: object\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from nltk.tokenize import RegexpTokenizer\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "\n",
                "# Load the dataset\n",
                "url = \"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\"\n",
                "df = pd.read_csv(url)\n",
                "\n",
                "# Define tokenizer, stopwords, and lemmatizer\n",
                "tokenizer = RegexpTokenizer(r'\\w+')\n",
                "stop_words = set(stopwords.words('english'))\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "# Function to preprocess a single URL\n",
                "def preprocess_url(url):\n",
                "    # Tokenize the URL\n",
                "    tokens = tokenizer.tokenize(url.lower())\n",
                "    # Remove stopwords and lemmatize tokens\n",
                "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
                "    return filtered_tokens\n",
                "\n",
                "# Apply preprocessing to all URLs in the dataset\n",
                "df['processed_url'] = df['url'].apply(preprocess_url)\n",
                "\n",
                "# Split the dataset into train and test sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(df['processed_url'], df['is_spam'], test_size=0.2, random_state=42)\n",
                "\n",
                "# Display the first few rows of the training set to verify the preprocessing\n",
                "print(X_train.head())\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Index(['url', 'is_spam', 'processed_url'], dtype='object')\n"
                    ]
                }
            ],
            "source": [
                "print(df.columns)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "       False       0.96      0.97      0.97       455\n",
                        "        True       0.91      0.88      0.89       145\n",
                        "\n",
                        "    accuracy                           0.95       600\n",
                        "   macro avg       0.93      0.92      0.93       600\n",
                        "weighted avg       0.95      0.95      0.95       600\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import classification_report\n",
                "\n",
                "# Convert processed URLs to strings\n",
                "X_train_str = X_train.apply(' '.join)\n",
                "X_test_str = X_test.apply(' '.join)\n",
                "\n",
                "# TF-IDF vectorization\n",
                "vectorizer = TfidfVectorizer()\n",
                "X_train_tfidf = vectorizer.fit_transform(X_train_str)\n",
                "X_test_tfidf = vectorizer.transform(X_test_str)\n",
                "\n",
                "# Initialize SVM classifier with default parameters\n",
                "svm_classifier = SVC()\n",
                "\n",
                "# Train the classifier\n",
                "svm_classifier.fit(X_train_tfidf, y_train)\n",
                "\n",
                "# Predictions on the test set\n",
                "y_pred = svm_classifier.predict(X_test_tfidf)\n",
                "\n",
                "# Analyze the results\n",
                "print(\"Classification Report:\")\n",
                "print(classification_report(y_test, y_pred))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Best Parameters: {'C': 3.0424224295953772, 'gamma': 'auto', 'kernel': 'linear'}\n",
                        "Classification Report (Best Model):\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "       False       0.98      0.95      0.96       455\n",
                        "        True       0.86      0.94      0.89       145\n",
                        "\n",
                        "    accuracy                           0.95       600\n",
                        "   macro avg       0.92      0.94      0.93       600\n",
                        "weighted avg       0.95      0.95      0.95       600\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.model_selection import RandomizedSearchCV\n",
                "from scipy.stats import uniform\n",
                "\n",
                "# Define the parameter grid\n",
                "param_grid = {\n",
                "    'C': uniform(loc=0, scale=10),  # Regularization parameter\n",
                "    'gamma': ['scale', 'auto'],     # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
                "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']  # Kernel type\n",
                "}\n",
                "\n",
                "# Initialize SVM classifier\n",
                "svm_classifier = SVC()\n",
                "\n",
                "# Initialize RandomizedSearchCV\n",
                "random_search = RandomizedSearchCV(svm_classifier, param_distributions=param_grid, n_iter=10, cv=5, random_state=42)\n",
                "\n",
                "# Perform random search\n",
                "random_search.fit(X_train_tfidf, y_train)\n",
                "\n",
                "# Best parameters found\n",
                "print(\"Best Parameters:\", random_search.best_params_)\n",
                "\n",
                "# Predictions on the test set using the best model\n",
                "best_svm = random_search.best_estimator_\n",
                "y_pred_best = best_svm.predict(X_test_tfidf)\n",
                "\n",
                "# Analyze the results of the best model\n",
                "print(\"Classification Report (Best Model):\")\n",
                "print(classification_report(y_test, y_pred_best))\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.13 64-bit ('3.8.13')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
